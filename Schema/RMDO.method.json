{
	"$schema": "http://json-schema.org/draft-07/schema#",
	"$id": "RMDO.method.json",
	"title": "Method",
	"description": "Papers With Code (https://paperswithcode.com/) Methods hierarchy as of 2024-04-30",
	"$comment": "RMDO:00118",
	"type": "string",
	"enum": [
		"(2+1)D Convolution",
		"1-bit Adam",
		"1-bit LAMB",
		"1cycle",
		"1D CNN",
		"1x1 Convolution",
		"2D DWT",
		"2D Parallel Distributed Methods",
		"3-Augment",
		"3D CNN",
		"3D Convolution",
		"3D Dynamic Scene Graph",
		"3D Face Mesh Models",
		"3D Gaussian Splatting",
		"3D Object Detection Models",
		"3D Reconstruction",
		"3D Representations",
		"3D ResNet-RS",
		"3D SA",
		"3DIS",
		"3DSSD",
		"4D A*",
		"6D Pose Estimation Models",
		"A2C",
		"A3C",
		"ABC",
		"ABCNet",
		"Absolute Position Encodings",
		"AccoMontage",
		"Accordion",
		"Accumulating Eligibility Trace",
		"Accuracy-Robustness Area (ARA)",
		"ACER",
		"ACGPN",
		"ACNN block",
		"Action Recognition Blocks",
		"Action Recognition Models",
		"Activation Functions",
		"Activation Normalization",
		"Activation Regularization",
		"Active Convolution",
		"Active Learning",
		"ACTKR",
		"Actor-Critic Algorithms",
		"AD-GCL",
		"ADA",
		"Adabelief",
		"AdaBound",
		"AdaDelta",
		"Adafactor",
		"AdaGPR",
		"AdaGrad",
		"AdaHessian",
		"Adam",
		"AdaMax",
		"AdaMod",
		"AdamW",
		"Adapter",
		"Adaptive Activation Functions",
		"Adaptive Computation",
		"Adaptive Dropout",
		"Adaptive Feature Pooling",
		"Adaptive Input Representations",
		"Adaptive Instance Normalization",
		"Adaptive Loss",
		"Adaptive Masking",
		"Adaptive NMS",
		"Adaptive Softmax",
		"Adaptive Span Transformer",
		"AdaptiveBins",
		"Adaptively Sparse Transformer",
		"AdaRNN",
		"AdaShift",
		"AdaSmooth",
		"AdaSqrt",
		"Additive Attention",
		"ADELE",
		"ADMM",
		"Adversarial Attacks",
		"Adversarial Color Enhancement",
		"Adversarial Image Data Augmentation",
		"Adversarial Soft Advantage Fitting (ASAF)",
		"Adversarial Solarization",
		"Adversarial Training",
		"AdvProp",
		"AE",
		"AEDA",
		"AffCorrs",
		"Affine Coupling",
		"Affine Operator",
		"Affinity Functions",
		"AGCN",
		"Agglomerative Contextual Decomposition",
		"AggMo",
		"Aggregated Learning",
		"Aging Evolution",
		"AHAF",
		"Air Quality Forecasting",
		"ALAE",
		"ALBEF",
		"ALBERT",
		"ALCN",
		"ALDA",
		"ALDEN",
		"ALDI++",
		"AlexNet",
		"ALI",
		"ALiBi",
		"ALIGN",
		"AlignPS",
		"ALIS",
		"All-Attention Layer",
		"ALP-GMM",
		"AlphaStar",
		"AlphaZero",
		"ALQ and AMQ",
		"ALS",
		"AltCLIP",
		"AltDiffusion",
		"AlterNet",
		"AM",
		"AmoebaNet",
		"AMP",
		"AMSBound",
		"AMSGrad",
		"Anchor Generation Modules",
		"Anchor Supervision",
		"Annealing SNNL",
		"Anti-Alias Downsampling",
		"Anycost GAN",
		"APA",
		"Ape-X DPG",
		"Ape-X DQN",
		"Ape-X",
		"Apollo",
		"APPNP",
		"APPO",
		"Approximate Inference",
		"Arbitrary Object Detectors",
		"ArcFace",
		"ARCH",
		"ARiA",
		"ARM-Net",
		"ARMA",
		"ARShoe",
		"ASAF",
		"ASFF",
		"ASLFeat",
		"ASPP",
		"Assemble-ResNet",
		"Associative LSTM",
		"ASU",
		"ASVI",
		"Asynchronous Data Parallel",
		"Asynchronous Interaction Aggregation",
		"Asynchronous Pipeline Parallel",
		"ATMO",
		"ATSS",
		"Attention Dropout",
		"Attention Feature Filters",
		"Attention Free Transformer",
		"Attention Gate",
		"Attention Mechanisms",
		"Attention Mesh",
		"Attention Modules",
		"Attention Patterns",
		"Attention Sinks",
		"Attention",
		"Attention-augmented Convolution",
		"Attentional Liquid Warping GAN",
		"Attentive Normalization",
		"AttLWB",
		"Attribute2Font",
		"AUCC",
		"AUCO ResNet",
		"Audio Artifact Removal",
		"Audio Model Blocks",
		"Audio",
		"Augmented Reality Methods",
		"Augmented SBERT",
		"AugMix",
		"Auto Parallel Methods",
		"Auto-Classifier",
		"AutoAugment",
		"AutoDropout",
		"AutoEncoder",
		"Autoencoding Transformers",
		"AutoGAN",
		"AutoGL",
		"AutoInt",
		"AutoML",
		"AutoML-Zero",
		"AutoParsimony",
		"Autoregressive Transformers",
		"AutoSmart",
		"AutoSync",
		"AutoTinyBERT",
		"Auxiliary Batch Normalization",
		"Auxiliary Classifier",
		"Average Pooling",
		"AVSlowFast",
		"AWARE",
		"AWD-LSTM",
		"Axial Attention",
		"b2b transfer learning",
		"Backbone Architectures",
		"BAGUA",
		"Balanced Feature Pyramid",
		"Balanced L1 Loss",
		"BAM",
		"Barlow Twins",
		"BART",
		"Base Boosting",
		"BASE",
		"BasicVSR",
		"BASNet",
		"Batch Normalization",
		"Batch Nuclear-norm Maximization",
		"Batchboost",
		"BatchChannel Normalization",
		"BatchFormer",
		"Bayesian Reinforcement Learning",
		"Bayesian REX",
		"Behaviour Policies",
		"BERT",
		"Beta-VAE",
		"BezierAlign",
		"Bi-attention",
		"Bi3D",
		"BIDeN",
		"BiDet",
		"Bidirectional Recurrent Neural Networks",
		"BiFPN",
		"Big-Little Module",
		"Big-Little Net",
		"BiGAN",
		"BigBiGAN",
		"BigBird",
		"BiGCN",
		"BiGG",
		"BigGAN",
		"BigGAN-deep",
		"BiGRU",
		"Bijective Transformation",
		"Bilateral Grid",
		"Bilateral Guided Aggregation Layer",
		"bilayer decoupling",
		"BiLSTM",
		"BIMAN",
		"Binary Neural Networks",
		"BinaryBERT",
		"BiSeNet V2",
		"BLANC",
		"Blended Diffusion",
		"Blender",
		"BlendMask",
		"Blink Communication",
		"BLIP",
		"BLOOM",
		"BLOOMZ",
		"Blue River Controls",
		"Board Game Models",
		"Boom Layer",
		"Boost-GNN",
		"Bort",
		"Bot Detection",
		"Bottleneck Residual Block",
		"Bottleneck Transformer Block",
		"Bottleneck Transformer",
		"Bottom-up Path Augmentation",
		"BoundaryNet",
		"BP-Transformer",
		"BPE",
		"Branch attention",
		"BRepNet",
		"Bridge-net",
		"BS-Net",
		"BTF",
		"BTmPG",
		"BYOL",
		"BytePS",
		"ByteScheduler",
		"C3-AMP",
		"CABiNet",
		"Cache Replacement Models",
		"CAD Design Models",
		"CAG",
		"CaiT",
		"Calculation of Price Graphs Reversal Point Using Average Regression Lines Method",
		"CAM",
		"CAMoE",
		"CANINE",
		"Canonical Partition",
		"Canvas Method",
		"CapsNet",
		"Capsule Network",
		"CARAFE",
		"Card Game Models",
		"CARLA",
		"Cascade Corner Pooling",
		"Cascade Mask R-CNN",
		"Cascade R-CNN",
		"CascadePSP",
		"Cashier-Free Shopping",
		"Categorical Modularity",
		"Causal Convolution",
		"CayleyNet",
		"CBAM",
		"CBHG",
		"CBNet",
		"CBoW Word2Vec",
		"CCAC",
		"CCNet",
		"CCT",
		"CDCC-NET",
		"CDEP",
		"CDIL-CNN",
		"CEAL",
		"CeiT",
		"CELU",
		"Center Pooling",
		"CenterMask",
		"CenterNet",
		"CenterPoint",
		"CenterTrack",
		"CentripetalNet",
		"CGMM",
		"CGNN",
		"CGRU",
		"Channel & Spatial attention",
		"Channel Attention Module",
		"Channel attention",
		"Channel Shuffle",
		"Channel Squeeze and Spatial Excitation",
		"Channel-wise Cross Attention",
		"Channel-wise Cross Fusion Transformer",
		"Channel-wise Soft Attention",
		"CharacterBERT",
		"Characteristic Functions",
		"Charformer",
		"ChebNet",
		"Checkerboard Dropout",
		"CheXNet",
		"Child-Tuning",
		"Chimera",
		"Chinchilla",
		"Chinese Pre-trained Unbalanced Transformer",
		"CHM",
		"CIDA",
		"CInC Flow",
		"CKConv",
		"ClariNet",
		"Class Activation Guided Attention Mechanism",
		"Class Attention",
		"Class-MLP",
		"classifier-guidance",
		"ClassSR",
		"CLIP",
		"ClipBERT",
		"CLIPort",
		"Clipped Double Q-learning",
		"CLRNet",
		"Cluster-GCN",
		"ClusterFit",
		"Clustering",
		"CMCL",
		"CNN BiLSTM",
		"Co-Correcting",
		"CoaT",
		"CoBERL",
		"COCO-FUNIT",
		"Code Generation Transformers",
		"CodeBERT",
		"CodeGen",
		"CodeSLAM",
		"CodeT5",
		"COLA",
		"Collaborative Distillation",
		"Colorization Transformer",
		"Colorization",
		"ColorJitter",
		"CoLU",
		"ComiRec",
		"Compact Global Descriptor",
		"ComplEx-N3",
		"ComplEx-N3-RP",
		"Composite Fields",
		"Compressed Memory",
		"Compressive Transformer",
		"Computation Redistribution",
		"Computer Vision",
		"Concatenated Skip Connection",
		"Concatenation Affinity",
		"Concrete Dropout",
		"Concurrent Spatial and Channel Squeeze & Excitation",
		"CondConv",
		"CondInst",
		"Conditional Batch Normalization",
		"Conditional DBlock",
		"Conditional Image-to-Image Translation Models",
		"Conditional Instance Normalization",
		"Conditional Positional Encoding",
		"Conffusion",
		"Confidence Calibration",
		"Confidence Estimators",
		"Content-based Attention",
		"Content-Conditioned Style Encoder",
		"Context Enhancement Module",
		"context2vec",
		"Contextual Residual Aggregation",
		"Contextualized Topic Models",
		"Contextualized Word Embeddings",
		"Contractive Autoencoder",
		"Contrastive Learning",
		"Contrastive Multiview Coding",
		"Contrastive Predictive Coding",
		"Control and Decision Systems",
		"ControlVAE",
		"ConvBERT",
		"Conversational Models",
		"ConViT",
		"ConvLSTM",
		"ConvMLP",
		"ConvNeXt",
		"Convolution",
		"Convolutional Neural Networks",
		"Convolutions",
		"ConvTasNet",
		"CoOp",
		"CoordConv",
		"Coordinate attention",
		"COP-KMeans",
		"Copy Mechanisms",
		"Copy-Paste",
		"CORAD",
		"Coresets",
		"Corner Pooling",
		"CornerNet",
		"CornerNet-Saccade",
		"CornerNet-Squeeze Hourglass Module",
		"CornerNet-Squeeze Hourglass",
		"CornerNet-Squeeze",
		"Cosine Annealing",
		"Cosine Normalization",
		"Cosine Power Annealing",
		"CosLU",
		"CoT Prompting",
		"Counterfactuals",
		"Counting Methods",
		"CoVA",
		"CoVe",
		"CoVR",
		"CP conv",
		"CP N3",
		"CP-N3",
		"CP-N3-RP",
		"CPC v2",
		"CPE",
		"CPM-2",
		"CPN",
		"CPVT",
		"CR-NET",
		"CReLU",
		"CRF",
		"CRF-RNN",
		"CRISS",
		"CRN",
		"Cross-Attention Module",
		"Cross-Covariance Attention",
		"Cross-encoder Reranking",
		"Cross-resolution features",
		"Cross-Scale Non-Local Attention",
		"Cross-View Training",
		"Crossbow",
		"CrossTransformers",
		"CrossViT",
		"CS-GAN",
		"CSGLD",
		"CSL",
		"CSPDarknet53",
		"CSPDenseNet",
		"CSPDenseNet-Elastic",
		"CSPPeleeNet",
		"CSPResNeXt Block",
		"CSPResNeXt",
		"CT-Layer",
		"CT3D",
		"CTAB-GAN",
		"CTAL",
		"CTC Loss",
		"CTracker",
		"CTRL",
		"CubeRE",
		"CuBERT",
		"CurricularFace",
		"CurvVAE",
		"CutBlur",
		"CutMix",
		"Cutout",
		"CV-MIM",
		"cVAE",
		"CVRL",
		"CvT",
		"CW-ERM",
		"Cycle Consistency Loss",
		"Cycle-CenterNet",
		"CycleGAN",
		"Cyclical Learning Rate Policy",
		"D4PG",
		"DABMD",
		"DAC",
		"DAEL",
		"DAFNe",
		"DAGNN",
		"DALL·E 2",
		"DAMO-YOLO",
		"DANCE",
		"DANet",
		"DAPO",
		"Darknet-19",
		"Darknet-53",
		"DARTS Max-W",
		"DARTS",
		"DASPP",
		"Data Parallel Methods",
		"DAU-ConvNet",
		"DBGAN",
		"DBlock",
		"DCGAN",
		"DCLS",
		"DCN-V2",
		"DCNN",
		"DD-PPO",
		"DDParser",
		"DDPG",
		"DDQL",
		"DDSP",
		"DE-GAN",
		"Deactivable Skip Connection",
		"DeBERTa",
		"DECA",
		"DeCLUTR",
		"DecomCAM",
		"Decorrelated Batch Normalization",
		"DeeBERT",
		"Deep Belief Network",
		"Deep Boltzmann Machine",
		"Deep Ensembles",
		"Deep LSTM Reader",
		"Deep Sets",
		"Deep Tabular Learning",
		"Deep Voice 3",
		"Deep-CAPTCHA",
		"Deep-MAC",
		"DeepCluster",
		"DeepDrug",
		"DeepIR",
		"DeepLab",
		"DeepLabv2",
		"DeepLabv3",
		"DeepMask",
		"DeepSIM",
		"DeepViT",
		"DeepWalk",
		"DeepZero",
		"Deflation",
		"Deformable Attention Module",
		"Deformable ConvNets",
		"Deformable Convolution",
		"Deformable DETR",
		"Deformable Kernel",
		"Deformable Position-Sensitive RoI Pooling",
		"Deformable RoI Pooling",
		"Degridding",
		"DeiT",
		"DELG",
		"DeLighT Block",
		"DeLighT",
		"DeltaConv",
		"DELU",
		"Demon ADAM",
		"Demon CM",
		"Demon",
		"Denoised Smoothing",
		"Denoising Autoencoder",
		"Denoising Score Matching",
		"Dense Block",
		"Dense Connections",
		"Dense Contrastive Learning",
		"Dense Synthesized Attention",
		"DenseNAS",
		"DenseNAS-A",
		"DenseNAS-B",
		"DenseNAS-C",
		"DenseNet",
		"DenseNet-Elastic",
		"Density Ratio Learning",
		"Dependency Parsers",
		"Depth-wise Plane Sweeping",
		"Depthwise Convolution",
		"Depthwise Dilated Separable Convolution",
		"Depthwise Fire Module",
		"Depthwise Separable Convolution",
		"DEQ",
		"Deraining Models",
		"Detection Assignment Rules",
		"DetNAS",
		"DetNASNet",
		"DetNet",
		"Detr",
		"DEXTR",
		"DExTra",
		"DFA",
		"DFDNet",
		"DG-Net",
		"DGCNN",
		"DGI",
		"DGRF",
		"Dialog Adaptation",
		"Dialog System Evaluation",
		"Dialogue State Trackers",
		"Dice Loss",
		"DiCE Unit",
		"DiCENet",
		"DiffAugment",
		"Differentiable Hyperparameter Search",
		"Differentiable NAS",
		"Differential attention for visual question answering",
		"Differential Diffusion",
		"DifferNet",
		"DiffPool",
		"Diffusion Models",
		"Diffusion",
		"Dilated Bottleneck Block",
		"Dilated Bottleneck with Projection Block",
		"Dilated Causal Convolution",
		"Dilated Convolution",
		"Dilated Sliding Window Attention",
		"DimConv",
		"DIME",
		"Dimensionality Reduction",
		"DimFuse",
		"DINO",
		"DIoU-NMS",
		"Directional Sparse Filtering",
		"discgabor",
		"Discrete Cosine Transform",
		"Discriminative Adversarial Search",
		"Discriminative Fine-Tuning",
		"Discriminative Regularization",
		"Discriminators",
		"Disentangled Attention Mechanism",
		"Disentangled Attribution Curves",
		"Disp R-CNN",
		"DistanceNet",
		"DistDGL",
		"DistilBERT",
		"Distillation",
		"Distributed Communication",
		"Distributed Methods",
		"Distributed Reinforcement Learning",
		"Distributed Shampoo",
		"Distribution Approximation",
		"Distributional Generalization",
		"Distributions",
		"DKL",
		"DLA",
		"DMA",
		"DMAGE",
		"DMVFN",
		"DNAS",
		"DNN2LR",
		"Document Embeddings",
		"Document Summary Evaluation",
		"Document Understanding Models",
		"DOLG",
		"Domain Adaptation",
		"Dorylus",
		"Dot-Product Attention",
		"Double DQN",
		"Double Q-learning",
		"DouZero",
		"Downsampling",
		"DPG",
		"DPN Block",
		"DPN",
		"DPT",
		"DQN",
		"Drafting Network",
		"Dreamix",
		"DROID-SLAM",
		"DropAttack",
		"DropBlock",
		"DropConnect",
		"Dropout",
		"DropPath",
		"DropPathway",
		"DRPNN",
		"DSAM loss",
		"DSelect-k",
		"DSGN",
		"DSPT",
		"DTW",
		"DU-GAN",
		"Dual Softmax Loss",
		"DualCL",
		"DualGCN",
		"Dueling Network",
		"Dutch Eligibility Trace",
		"DV3 Attention Block",
		"DV3 Convolution Block",
		"DVD-GAN DBlock",
		"DVD-GAN GBlock",
		"DVD-GAN",
		"DyGED",
		"DynaBERT",
		"Dynamic Convolution",
		"Dynamic Keypoint Head",
		"Dynamic Memory Network",
		"Dynamic R-CNN",
		"Dynamic SmoothL1 Loss",
		"DynamicConv",
		"E-Branchformer",
		"E-MBConv",
		"E-swish",
		"E2EAdaptiveDistTraining",
		"Early Dropout",
		"Early exiting",
		"Early Stopping",
		"EBC",
		"EBM",
		"ECA-Net",
		"ECANet",
		"ED-GNN",
		"EdgeBoxes",
		"EdgeFlow",
		"EDLPS",
		"EEND",
		"EESP",
		"Effective Squeeze-and-Excitation Block",
		"Efficient Channel Attention",
		"Efficient Planning",
		"EfficientDet",
		"EfficientNet",
		"EfficientNetV2",
		"EfficientUNet++",
		"EGT",
		"Elastic Dense Block",
		"Elastic ResNeXt Block",
		"ElasticFace",
		"ELECTRA",
		"Electric",
		"Eligibility Trace",
		"Eligibility Traces",
		"ELiSH",
		"ELMo",
		"ELR",
		"ELU",
		"Embedded Dot Product Affinity",
		"Embedded Gaussian Affinity",
		"Embedding Dropout",
		"EmbraceNet",
		"EMEA",
		"EMF",
		"EMQAP",
		"End-To-End Memory Network",
		"Energy Based Process",
		"ENet Bottleneck",
		"ENet Dilated Bottleneck",
		"ENet Initial Block",
		"ENet",
		"Enhanced Fusion Framework",
		"ENIGMA",
		"Ensemble Clustering",
		"Ensembling",
		"Entity Recognition Models",
		"Entity Retrieval Models",
		"Entropy Regularization",
		"Environment Design Methods",
		"Epsilon Greedy Exploration",
		"ERNIE",
		"ERNIE-GEN",
		"ERU",
		"ESACL",
		"ESIM",
		"ESP",
		"ESPNet",
		"ESPNetv2",
		"Estimation Statistics",
		"EsViT",
		"ETC",
		"Euclidean Norm Regularization",
		"EVM",
		"EvoNorms",
		"EWC",
		"Exact Fusion Model",
		"Exaggeration Detection Models",
		"Expected Sarsa",
		"Experience Replay",
		"Explainable CNNs",
		"Explanation vs Attention",
		"Exploration Strategies",
		"Exponential Decay",
		"ExtremeNet",
		"F2DNet",
		"FA",
		"Face Detection Models",
		"Face Privacy",
		"Face Recognition Models",
		"Face Restoration Models",
		"Face-to-Face Translation",
		"Factorization Machines",
		"Factorized Dense Synthesized Attention",
		"Factorized Random Synthesized Attention",
		"FairMOT",
		"FASFA",
		"FashionCLIP",
		"Fast AutoAugment",
		"Fast Minimum-Norm Attack",
		"Fast R-CNN",
		"Fast Sample Re-Weighting",
		"Fast Voxel Query",
		"Fast-OCR",
		"Fast-YOLOv2",
		"Fast-YOLOv3",
		"Fast-YOLOv4-SmallObj",
		"Fast_BAT",
		"Faster R-CNN",
		"Fastformer",
		"FastGCN",
		"FastMoE",
		"FastPitch",
		"FastSGT",
		"FastSpeech 2",
		"FastSpeech 2s",
		"fastText",
		"FAVOR+",
		"Fawkes",
		"FBNet Block",
		"FBNet",
		"FcaNet",
		"FCN",
		"FCOS",
		"FCPose",
		"Feature Extractors",
		"Feature Intertwiner",
		"Feature Matching",
		"Feature Pyramid Blocks",
		"Feature Selection",
		"Feature Upsampling",
		"Feature-Centric Voting",
		"FeatureNMS",
		"Feedback Memory",
		"Feedback Transformer",
		"Feedforward Network",
		"Feedforward Networks",
		"FEFM",
		"FEM",
		"Few-Shot Image-to-Image Translation",
		"FFB6D",
		"FFF",
		"FFMv1",
		"FFMv2",
		"FGA",
		"FIERCE",
		"FiLM Module",
		"Filter Response Normalization",
		"FINCH Clustering",
		"Fine-Tuning",
		"Fire Module",
		"Firefly algorithm",
		"Fisher-BRC",
		"Fishr",
		"Fixed Factorized Attention",
		"FixMatch",
		"FixRes",
		"Fixup Initialization",
		"Flan-T5",
		"FLAVA",
		"FLAVR",
		"FlexFlow",
		"FLICA",
		"FLIP",
		"Florence",
		"Flow Alignment Module",
		"Flow Matching",
		"Flow Normalization",
		"FM with splines",
		"FMix",
		"Focal Loss",
		"Focal Transformers",
		"Focus",
		"Font Generation Models",
		"FORK",
		"Forward gradient",
		"Fourier Contour Embedding",
		"Fourier-related Transforms",
		"FoveaBox",
		"FPG",
		"FPN",
		"Fractal Block",
		"FractalNet",
		"Fragmentation",
		"Fraternal Dropout",
		"FreeAnchor",
		"FRILL",
		"FSAF",
		"FT-Transformer",
		"Funnel Transformer",
		"FuseFormer Block",
		"FuseFormer",
		"Fuzzy Logic",
		"G-GLN Neuron",
		"G-GLN",
		"G-NIA",
		"G3D",
		"GA",
		"GA-PID/NN-PID",
		"GaAN",
		"GAGNN",
		"GAIL",
		"GALA",
		"Galactica",
		"GAM",
		"GAN Feature Matching",
		"GAN Hinge Loss",
		"GAN Least Squares Loss",
		"GAN",
		"GAN-TTS",
		"GANformer",
		"GAP-Layer",
		"GAT",
		"Gated Convolution Network",
		"Gated Convolution",
		"Gated Linear Networks",
		"Gather-Excite Networks",
		"GATv2",
		"Gaussian Affinity",
		"Gaussian Process",
		"GBlock",
		"GBO",
		"GCA",
		"gCANS",
		"GCN",
		"GCNet",
		"GCNFN",
		"GCNII",
		"GCT",
		"GCU",
		"GECO",
		"GEE",
		"GeGLU",
		"GELU",
		"General",
		"Generalization",
		"Generalized Additive Models",
		"Generalized Focal Loss",
		"Generalized Linear Models",
		"Generalized Mean Pooling",
		"Generative Adversarial Networks",
		"Generative Audio Models",
		"Generative Discrimination",
		"Generative Models",
		"Generative Sequence Models",
		"Generative Training",
		"Generative Video Models",
		"GENet",
		"Genetic Algorithm",
		"GeniePath",
		"GenSAM",
		"GEOMANCER",
		"Geometric Matching",
		"GFP-GAN",
		"GFSA",
		"GGS-NNs",
		"GHM-C",
		"GHM-R",
		"Ghost Bottleneck",
		"Ghost Module",
		"GhostNet",
		"GIC",
		"GIN",
		"GLIDE",
		"GLM",
		"GLN",
		"Global and Sliding Window Attention",
		"Global Average Pooling",
		"Global Context Block",
		"Global Context Modules",
		"Global Convolutional Network",
		"Global Local Attention Module",
		"Global Sub-Sampled Attention",
		"Global-Local Attention",
		"GloVe",
		"GLOW",
		"Glow-TTS",
		"GLU",
		"GMI",
		"gMLP",
		"GMVAE",
		"GNNCL",
		"GNS",
		"Go-Explore",
		"Good Feature Matching",
		"GoogLeNet",
		"GPFL",
		"GPipe",
		"GPS",
		"GPSA",
		"GPT",
		"GPT-2",
		"GPT-3",
		"GPT-4",
		"GPT-Neo",
		"GPT-NeoX",
		"Grab",
		"GradDrop",
		"Gradient Checkpointing",
		"Gradient Clipping",
		"Gradient Normalization",
		"Gradient Sparsification",
		"Gradient-Based Subword Tokenization",
		"GradientDICE",
		"Gradual Self-Training",
		"Grammatical evolution + Q-learning",
		"GRANDE",
		"Graph Contrastive Coding",
		"Graph Data Augmentation",
		"Graph Embeddings",
		"Graph Models",
		"Graph Neural Network",
		"Graph Representation Learning",
		"Graph Self-Attention",
		"Graph Transformer",
		"Graph2Tree",
		"GraphCL",
		"GraphESN",
		"Graphics Models",
		"Graphs",
		"GraphSAGE",
		"GraphSAINT",
		"GraRep",
		"Gravity",
		"GreedyNAS",
		"GreedyNAS-A",
		"GreedyNAS-B",
		"GreedyNAS-C",
		"Grid R-CNN",
		"Grid Sensitive",
		"GridMask",
		"Griffin-Lim Algorithm",
		"GRIN",
		"GRLIA",
		"GRoIE",
		"Group Normalization",
		"GroupDNet",
		"Grouped Convolution",
		"Grouped-query attention",
		"Groupwise Point Convolution",
		"GrowNet",
		"GRU",
		"gSDE",
		"GShard",
		"GSoP-Net",
		"GTrXL",
		"GTS",
		"Guided Anchoring",
		"Gumbel Activation",
		"Gumbel Softmax",
		"H-BEMD",
		"H3DNet",
		"HalluciNet",
		"HaloNet",
		"Hamburger",
		"Handwritten OCR",
		"HANet",
		"HAPPIER",
		"Hard Sigmoid",
		"Hard Swish",
		"HardELiSH",
		"Hardtanh Activation",
		"Harm-Net",
		"Harmonic Block",
		"Harris Hawks optimization (HHO)",
		"HBMP",
		"HDCGAN",
		"hdxresnet",
		"Heatmap",
		"HEGCN",
		"Hermite Activation",
		"Herring",
		"HetPipe",
		"Heuristic Search Algorithms",
		"HFPSO",
		"HGS",
		"Hi-LANDER",
		"Hierarchical Feature Fusion",
		"Hierarchical MTL",
		"Hierarchical Network Dissection",
		"Hierarchical Softmax",
		"Hierarchical VAE",
		"Hierarchical-Split Block",
		"HiFi-GAN",
		"High-level backbone",
		"High-resolution input",
		"Highway Layer",
		"Highway Network",
		"Highway networks",
		"HiSD",
		"Hit-Detector",
		"HITNet",
		"HMGNN",
		"HOC",
		"Holographic Reduced Representation",
		"HOPE",
		"Hopfield Layer",
		"Hourglass Module",
		"HPO",
		"HRank",
		"HRI pipeline",
		"HRNet",
		"HS-ResNet",
		"HTC",
		"HTCN",
		"Huber loss",
		"Human Object Interaction Detectors",
		"Hybrid AWT",
		"Hybrid Fuzzing",
		"Hybrid Optimization",
		"Hybrid Parallel Methods",
		"Hydra",
		"HypE",
		"HyperDenseNet",
		"HyperHyperNetwork",
		"HyperNetwork",
		"Hyperparameter Search",
		"HyperSA",
		"HyperTree MetaModel",
		"I-BERT",
		"I3DR-Net",
		"IAN",
		"IB-BERT",
		"IC-SBP",
		"ICA",
		"IFBlock",
		"IFNet",
		"iGCL",
		"IGSA",
		"IICNet",
		"IkshanaNet",
		"ILVR",
		"Image Colorization Models",
		"Image Data Augmentation",
		"Image dataset comparison metric",
		"Image Decomposition Models",
		"Image Denoising Models",
		"Image Feature Extractors",
		"Image Generation Models",
		"Image Inpainting Modules",
		"Image Manipulation Models",
		"Image Model Blocks",
		"Image Models",
		"Image Quality Models",
		"Image Representations",
		"Image Restoration Models",
		"Image Retrieval Models",
		"Image Scale Augmentation",
		"Image Scaling Strategies",
		"Image Segmentation Models",
		"Image Semantic Segmentation Metric",
		"Image Super-Resolution Models",
		"imagemorph",
		"IMGEP",
		"imGHUM",
		"Imitation Learning Methods",
		"IMPALA",
		"Implicit PointRend",
		"Inception Module",
		"Inception v2",
		"Inception-A",
		"Inception-B",
		"Inception-C",
		"Inception-ResNet-v2 Reduction-B",
		"Inception-ResNet-v2",
		"Inception-ResNet-v2-A",
		"Inception-ResNet-v2-B",
		"Inception-ResNet-v2-C",
		"Inception-v3 Module",
		"Inception-v3",
		"Inception-v4",
		"InceptionTime",
		"Incident Aggregation Models",
		"Inference Attack",
		"Inference Engines",
		"Inference Extrapolation",
		"InfoGAN",
		"InfoGraph",
		"InfoNCE",
		"Information Bottleneck",
		"Information Retrieval Methods",
		"Informative Sample Mining Network",
		"Initialization",
		"Inpainting",
		"InPlace-ABN",
		"Input Embedding Factorization",
		"InstaBoost",
		"Instance Normalization",
		"Instance Segmentation Models",
		"Instance Segmentation Modules",
		"Instance-Level Meta Normalization",
		"Interactive Semantic Segmentation Models",
		"InterBERT",
		"Internet Explorer",
		"InternVideo",
		"Interpretability",
		"intgauss",
		"intgaussder",
		"Intra-Layer Parallel",
		"Inverse Square Root Schedule",
		"Inverted Residual Block",
		"Invertible 1x1 Convolution",
		"Invertible NxN Convolution",
		"Involution",
		"IoU-Balanced Sampling",
		"IoU-guided NMS",
		"IoU-Net",
		"IPA-GNN",
		"IPBI",
		"IPL",
		"IQ-Learn",
		"IQL",
		"IRN",
		"ISPL",
		"IterInpaint",
		"JDeskew",
		"Jigsaw",
		"JLA",
		"Jukebox",
		"K-Maximal Word Allocation",
		"k-Means Clustering",
		"K-Net",
		"k-NN",
		"k-Sparse Autoencoder",
		"K3M",
		"KAF",
		"Kaiming Initialization",
		"Kaleido-BERT",
		"KE-MLM",
		"Kernel Methods",
		"KGRefiner",
		"KIP",
		"KNN and IOU based verification",
		"Knowledge Distillation",
		"KnowPrompt",
		"KOVA",
		"KP",
		"KPE",
		"KungFu",
		"L-GCN",
		"L1 Regularization",
		"L2M",
		"Label Correction",
		"Label Quality Model",
		"Label Smoothing",
		"LAMA",
		"LAMB",
		"Lambda Layer",
		"Lane Detection Models",
		"Language Model Components",
		"Language Model Pre-Training",
		"Language Models",
		"LapEigen",
		"LAPGAN",
		"Laplacian PE",
		"Laplacian Pyramid",
		"LapStyle",
		"Large Batch Optimization",
		"Large-scale spectral clustering",
		"LARS",
		"Latent Diffusion Model",
		"Latent Optimisation",
		"Latent Variable Sampling",
		"Layer Normalization",
		"LayerDrop",
		"LayerScale",
		"Layout Annotation Models",
		"LayoutLMv2",
		"LayoutReader",
		"Lbl2TransformerVec",
		"Lbl2Vec",
		"LCC",
		"LDA",
		"lda2vec",
		"Leadership Inference",
		"LEAF",
		"Leaky ReLU",
		"Learning Rate Schedules",
		"Learning to Rank Models",
		"Lecun's Tanh",
		"LeNet",
		"Levenshtein Transformer",
		"Leverage Learning",
		"LeViT Attention Block",
		"LeVIT",
		"LFME",
		"LFPNet (TTA)",
		"LGCL",
		"Libra R-CNN",
		"Lifelong Learning",
		"Light-weight neural networks",
		"LightAutoML",
		"LightConv",
		"LightGCN",
		"Likelihood-Based Generative Models",
		"LIME",
		"LIMix",
		"LinComb",
		"LINE",
		"Linear Layer",
		"Linear Regression",
		"Linear Warmup With Cosine Annealing",
		"Linear Warmup With Linear Decay",
		"Linear Warmup",
		"Linformer",
		"Lion",
		"LipGAN",
		"LiteSeg",
		"LLaMA",
		"LMOT",
		"LMU",
		"Local Augmentation",
		"Local Contrast Normalization",
		"Local Importance-based Pooling",
		"Local Mixup",
		"Local Patch Interaction",
		"Local Relation Layer",
		"Local Response Normalization",
		"Local SGD",
		"Localization Models",
		"Locally-Grouped Self-Attention",
		"LocalViT",
		"Location Sensitive Attention",
		"Location-based Attention",
		"Log Decay",
		"LOGAN",
		"Logistic Regression",
		"Long-Range Interaction Layers",
		"Longformer",
		"Lookahead",
		"Loss Functions",
		"Lovasz-Softmax",
		"Low Rank Tensor Learning Paradigms",
		"Low-level backbone",
		"Low-resolution input",
		"Lower Bound on Transmission using Non-Linear Bounding Function in Single Image Dehazing",
		"LPM",
		"LR-Net",
		"LRNet",
		"LSDM",
		"LSGAN",
		"LSH Attention",
		"LSTM",
		"LSUV Initialization",
		"LTLS",
		"LV-ViT",
		"LXMERT",
		"m-arcsinh",
		"M-S structure",
		"M2Det",
		"M3L",
		"Macaw",
		"MacBERT",
		"MACEst",
		"Machine Translation Models",
		"MAD Learning",
		"MADDPG",
		"MADGRAD",
		"MAE",
		"MagFace",
		"Magnification Prior Contrastive Similarity",
		"Make-A-Scene",
		"Mamba",
		"MAML",
		"Manifold Disentangling",
		"Manifold Mixup",
		"ManifoldPlus",
		"Margin ReLU",
		"Markov Chain Monte Carlo",
		"MARLIN",
		"MAS",
		"Mask Branches",
		"Mask R-CNN",
		"Mask Scoring R-CNN",
		"Masked Convolution",
		"MaskFlownet",
		"MATE",
		"Math Formula Detection Models",
		"Matrix NMS",
		"MatrixNet",
		"MAVL",
		"Max Pooling",
		"Maxout",
		"MaxUp",
		"mBART",
		"mBARTHez",
		"mBERT",
		"MBS",
		"MCKERNEL",
		"MDETR",
		"MDL",
		"MDPO",
		"MDTVSFA",
		"Mechanism Transfer",
		"Medical Image Deblurring",
		"Medical Image Models",
		"Medical waveform analysis",
		"Meena",
		"MEI",
		"MelGAN Residual Block",
		"MelGAN",
		"Memory Network",
		"MEND",
		"MeRL",
		"Mesh-Based Simulation Models",
		"Mesh-TensorFlow",
		"MeshGraphNet",
		"Meshing",
		"Meta Pseudo Labels",
		"Meta-augmentation",
		"Meta-Learning Algorithms",
		"MetaFormer",
		"Metrix",
		"Metropolis Hastings",
		"MEUZZ",
		"MFEC",
		"MFF",
		"MFR",
		"MHMA",
		"MIM",
		"MinCutPool",
		"Minibatch Discrimination",
		"Mirror-BERT",
		"Miscellaneous Components",
		"Mish",
		"MiVOS",
		"Mix-FFN",
		"MixConv",
		"Mixed Attention Block",
		"Mixer Layer",
		"MixNet",
		"MixText",
		"Mixture Normalization",
		"Mixture of Logistic Distributions",
		"Mixture of Softmaxes",
		"Mixture-of-Experts",
		"Mixup",
		"MLFPN",
		"MLP-Mixer",
		"mLSTM",
		"MnasNet",
		"MNMF",
		"Mobile DenseNet",
		"Mobile Neural Network",
		"MobileBERT",
		"MobileDet",
		"MobileNetV1",
		"MobileNetV2",
		"MobileNetV3",
		"MobileViT",
		"MoBY",
		"MoCo v2",
		"MoCo v3",
		"MoCo",
		"Mode Normalization",
		"Model Compression",
		"Model Parallel Methods",
		"Models Genesis",
		"MODERN",
		"MODNet",
		"modReLU",
		"MoE",
		"MoGA-A",
		"MoGA-B",
		"MoGA-C",
		"Mogrifier LSTM",
		"Momentum Rules",
		"MoNet",
		"Monocular Depth Estimation Models",
		"MonoPort",
		"Monte Carlo Dropout",
		"Monte-Carlo Tree Search",
		"Morphence",
		"Motion Control",
		"Motion Disentanglement",
		"Motion Prediction Models",
		"MotionNet",
		"Movement Pruning",
		"MoViNet",
		"MPCK-Means",
		"MPN",
		"MPNet",
		"MPNN",
		"MPRNet",
		"MPSO",
		"mRNN",
		"MSGAN",
		"MSPFN",
		"MT-PET",
		"mT0",
		"mT5",
		"MTransE",
		"MTS",
		"Multi Loss ( BCE Loss + Focal Loss )  + Dice Loss",
		"Multi-Attention Network",
		"Multi-band MelGAN",
		"Multi-DConv-Head Attention",
		"Multi-Head Attention",
		"Multi-Head Linear Attention",
		"Multi-Modal Methods",
		"Multi-Object Tracking Models",
		"Multi-Query Attention",
		"Multi-scale analysis",
		"Multi-Scale Training",
		"MultiGrain",
		"MultiKE",
		"Multiple Random Window Discriminator",
		"Multiplicative Attention",
		"Multiscale Dilated Convolution Block",
		"MushroomRL",
		"Music source separation",
		"Music Transcription",
		"MUSIQ",
		"MutualGuide",
		"MuVER",
		"MuZero",
		"MViT",
		"MXMNet",
		"myGym",
		"N-step Returns",
		"NADAM",
		"NAFNet",
		"NAM",
		"NAS-FCOS",
		"NAS-FPN",
		"Natural Gradient Descent",
		"Natural Language Processing",
		"NCL",
		"NEAT",
		"Negative Sampling",
		"Neighborhood Attention",
		"NeRF",
		"NesT",
		"Nesterov Accelerated Gradient",
		"NetAdapt",
		"NetMF",
		"Network Dissection",
		"Network Shrinking",
		"Neural adjoint",
		"Neural Architecture Search",
		"Neural Cache",
		"Neural Probabilistic Language Model",
		"Neural Tangent Transfer",
		"Neural Turing Machine",
		"NeuralRecon",
		"NeuroTactic",
		"NFN",
		"NFR",
		"NICE",
		"NICE-SLAM",
		"NIMA",
		"Nipuna",
		"NLSIG",
		"NN4G",
		"NNCF",
		"NNCLR",
		"nnFormer",
		"NODE",
		"node2vec",
		"Noise2Fast",
		"Noisy Linear Layer",
		"Noisy Student",
		"NoisyNet-A3C",
		"NoisyNet-DQN",
		"NoisyNet-Dueling",
		"Non Maximum Suppression",
		"NON",
		"Non-Linear-Bounding-Function",
		"Non-Local Block",
		"Non-Local Operation",
		"Non-Parametric Classification",
		"Non-Parametric Regression",
		"Normalization",
		"Normalizing Flows",
		"NormFormer",
		"NormLinComb",
		"NPID",
		"NPID++",
		"NT-ASGD",
		"NT-Xent",
		"NTK",
		"NUQSGD",
		"NVAE Encoder Residual Cell",
		"NVAE Generative Residual Cell",
		"NVAE",
		"Nyströmformer",
		"O-Net",
		"OA-Loss",
		"OA-Mix",
		"OASIS",
		"Object Detection Models",
		"Object Detection Modules",
		"Object Dropout",
		"OCD",
		"OCR Models",
		"Octave Convolution",
		"ODL",
		"OFA",
		"Off-Diagonal Orthogonal Regularization",
		"Off-Policy TD Control",
		"Offline Reinforcement Learning Methods",
		"OHEM",
		"OMGD",
		"On-Policy TD Control",
		"One-Shot Aggregation",
		"One-Stage Object Detection Models",
		"OneR",
		"Online Normalization",
		"Ontology",
		"OODformer",
		"ooJpiued",
		"Open-Domain Chatbots",
		"OpenPose",
		"OPT",
		"OPT-IML",
		"Optimization",
		"ORB-SLAM2",
		"Oriented Object Detection Models",
		"ORN",
		"Orthogonal Regularization",
		"OSA (identity mapping + eSE)",
		"OSCAR",
		"OTM",
		"Out-of-Distribution Example Detection",
		"Output Functions",
		"Output Heads",
		"OverFeat",
		"PAA",
		"Packed Levitated Markers",
		"PAFNet",
		"PAFPN",
		"PAFs",
		"PAG",
		"PALED",
		"PaLM",
		"PANet",
		"PanGu-$α$",
		"PanNet",
		"Panoptic FPN",
		"Panoptic-PolarNet",
		"PAR Transformer",
		"Parallax",
		"Parallel Layers",
		"ParamCrop",
		"Parameter Norm Penalties",
		"Parameter Server Methods",
		"Parameter Sharing",
		"Parametric UMAP",
		"ParaNet Convolution Block",
		"ParaNet",
		"Paraphrase Generation Models",
		"Parrot",
		"Partition Filter Network",
		"PASE+",
		"Passage Re-Ranking Models",
		"Patch Merger",
		"PatchAugment",
		"PatchGAN",
		"Path Length Regularization",
		"Path Planning",
		"Pattern-Exploiting Training",
		"PAU",
		"PAUSE",
		"PCA Whitening",
		"PCA",
		"PCB",
		"PCIDA",
		"PCKMeans",
		"PCT",
		"PDC",
		"Peer-attention",
		"PEGASUS",
		"PeleeNet",
		"PELU",
		"Performer",
		"PermuteFormer",
		"Person Search Models",
		"Perturbed-Attention Guidance",
		"PFGM",
		"PFPNet",
		"PGC-DGCNN",
		"PGHI",
		"PGM",
		"PGNet",
		"Phase Reconstruction",
		"Phase Shuffle",
		"Phish",
		"PICARD",
		"PinvGCN",
		"PIoU Loss",
		"PipeDream",
		"PipeDream-2BW",
		"Pipelined Backpropagation",
		"PipeMare",
		"PipeTransformer",
		"PIRL",
		"PISA",
		"Pix2Pix",
		"Pixel Tracking",
		"Pixel-BERT",
		"pixel2style2pixel",
		"PixelCNN",
		"PixelRNN",
		"PixelShuffle",
		"PixLoc",
		"PLATO-2",
		"Playstyle Distance",
		"Playstyle",
		"PLIP",
		"PMLM",
		"PNA",
		"PNAS",
		"PnP",
		"PO3D-VQA",
		"PoAPL",
		"PocketNet",
		"Poincaré Embeddings",
		"Point Cloud Augmentation",
		"Point Cloud Models",
		"Point Cloud Representations",
		"Point-GNN",
		"Point-wise Spatial Attention",
		"PointASNL",
		"PointAugment",
		"Pointer Network",
		"Pointer Sentinel-LSTM",
		"PointNet",
		"PointRend",
		"Pointwise Convolution",
		"PolarMask",
		"PolarNet",
		"Policy Evaluation",
		"Policy Gradient Methods",
		"Policy Similarity Metric",
		"Poly",
		"Polya-Gamma Augmentation",
		"Polyak Averaging",
		"PolyCAM",
		"PolyConv",
		"Polynomial Rate Decay",
		"POMO",
		"PonderNet",
		"PoolFormer",
		"Pooling Operations",
		"Population Based Augmentation",
		"Population Based Training",
		"Portrait Matting Models",
		"Pose Contrastive Learning",
		"Pose Disentangling",
		"Pose Estimation Blocks",
		"Pose Estimation Models",
		"Position Embeddings",
		"Position Recovery Models",
		"Position-Sensitive RoI Pooling",
		"Position-Sensitive RoIAlign",
		"Position-Wise Feed-Forward Layer",
		"Positional Encoding Generator",
		"POTO",
		"Powerpropagation",
		"PowerSGD",
		"PP-OCR",
		"PP-YOLO",
		"PP-YOLOv2",
		"PPMC",
		"PPO",
		"PQ-Transformer",
		"Precise RoI Pooling",
		"PREDATOR",
		"PReLU",
		"PReLU-Net",
		"PresGAN",
		"Primer",
		"Prioritized Experience Replay",
		"Prioritized Sampling",
		"Prioritized Sweeping",
		"PrivacyNet",
		"PRNet+",
		"Probabilistic Anchor Assignment",
		"ProCAN",
		"Procrustes",
		"ProGAN",
		"Projection Discriminator",
		"Prompt Engineering",
		"ProphetNet",
		"Proposal Filtering",
		"Protagonist Antagonist Induced Regret Environment Design",
		"Proximity Regularization",
		"ProxyAnchorLoss",
		"ProxylessNAS",
		"ProxylessNet-CPU",
		"ProxylessNet-GPU",
		"ProxylessNet-Mobile",
		"ProxyOptimization",
		"Pruning",
		"PSANet",
		"PSFR-GAN",
		"PSPNet",
		"PULSE",
		"PVT",
		"PVTv2",
		"PWIL",
		"Pyramid Pooling Module",
		"Pyramidal Bottleneck Residual Unit",
		"Pyramidal Residual Unit",
		"PyramidNet",
		"Pythia",
		"PyTorch DDP",
		"Q-Learning Networks",
		"Q-Learning",
		"QHAdam",
		"QHM",
		"QPT",
		"QRNN",
		"QuantTree",
		"Quantum Methods",
		"Question Answering Models",
		"Quick Attention",
		"R(2+1)D",
		"R-CNN",
		"R-FCN",
		"R-Mix",
		"R1 Regularization",
		"R2D2",
		"RAdam",
		"RAE",
		"RAG",
		"RAHP",
		"Rainbow DQN",
		"RAM",
		"RAN",
		"RandAugment",
		"Random Erasing",
		"Random Gaussian Blur",
		"Random Grayscale",
		"Random Horizontal Flip",
		"Random Mutation Search",
		"Random Resized Crop",
		"Random Scaling",
		"Random Search",
		"Random Synthesized Attention",
		"Randomized Deletion",
		"Randomized Smoothing",
		"Randomized Value Functions",
		"RandomRotate",
		"RandSol",
		"RandWire",
		"Rank-based Loss",
		"Rational Activation function",
		"Rational Activation Function",
		"RBF",
		"RBPN",
		"RDF2Vec",
		"Re-Attention Module",
		"RE-NET",
		"Reading Comprehension Models",
		"Reading Order Detection Models",
		"RealFormer",
		"RealNVP",
		"ReasonBERT",
		"Recommendation Systems",
		"Recurrent Dropout",
		"Recurrent Entity Network",
		"Recurrent Neural Networks",
		"Reduction-A",
		"Reduction-B",
		"Reformer",
		"ReGaDa",
		"Region Proposal",
		"RegionViT",
		"ReGLU",
		"RegNetX",
		"RegNetY",
		"Regularization",
		"REINFORCE",
		"Reinforcement Learning Frameworks",
		"Reinforcement Learning",
		"ReInfoSelect",
		"Relation Extraction Models",
		"Relative Position Encodings",
		"Relativistic GAN",
		"RelDiff",
		"Reliability Balancing",
		"ReLIC",
		"ReLU",
		"ReLU6",
		"ReLUN",
		"REM",
		"Rendezvous",
		"Replacing Eligibility Trace",
		"Replay Memory",
		"Replicated Data Parallel",
		"RepPoints",
		"Representation Learning",
		"RepVGG",
		"Res2Net Block",
		"Res2Net",
		"RESCAL RP",
		"RESCAL",
		"RESCAL-RP",
		"reSGLD",
		"Residual Block",
		"Residual Connection",
		"Residual GRU",
		"Residual Normal Distribution",
		"Residual SRM",
		"ResMLP",
		"ResNeSt",
		"ResNet",
		"ResNet-D",
		"ResNet-RS",
		"ResNeXt Block",
		"ResNeXt",
		"ResNeXt-Elastic",
		"Restricted Boltzmann Machine",
		"RetinaMask",
		"RetinaNet",
		"RetinaNet-RS",
		"Retrace",
		"Reversible Image Conversion Models",
		"Reversible Residual Block",
		"Revision Network",
		"RevNet",
		"RevSilo",
		"ReZero",
		"RFB Net",
		"RFB",
		"RFE",
		"RFP",
		"RGA",
		"RGB-D Saliency Detection Models",
		"RGCN",
		"RIFE",
		"RL Transformers",
		"RMN",
		"RMS Pooling",
		"RMSNorm",
		"RMSProp",
		"rnnDrop",
		"RoBERTa",
		"Robotic Manipulation Models",
		"Robust Predictable Control",
		"Robust Training",
		"Robustness Methods",
		"ROCKET",
		"RoI Feature Extractors",
		"RoI Tanh-polar Transform",
		"RoIAlign",
		"RoIPool",
		"RoIWarp",
		"ROME",
		"Rotary Embeddings",
		"RotatE",
		"RotNet",
		"Routing Attention",
		"Routing Transformer",
		"RPDet",
		"RPM-Net",
		"RPN",
		"RREA",
		"RReLU",
		"RSE",
		"RSU Beneš Block",
		"RTMDet",
		"rTPNN",
		"Rule Learners",
		"Rule-based systems",
		"RUN",
		"S-GCN",
		"SABL",
		"SaBN",
		"SAC",
		"SAENet",
		"Safety-llamas",
		"SAFRAN",
		"SAG",
		"SAGA",
		"SAGAN Self-Attention Module",
		"SAGAN",
		"SAINT",
		"SAM",
		"Sample Re-Weighting",
		"Sample Redistribution",
		"Sandwich Transformer",
		"SANet",
		"Sarsa Lambda",
		"Sarsa",
		"SASA",
		"SBERT",
		"SC-GPT",
		"SCA",
		"SCA-CNN",
		"Scale Aggregation Block",
		"Scaled Dot-Product Attention",
		"ScaledSoftSign",
		"ScaleNet",
		"SCAN-clustering",
		"ScanSSD",
		"SCARF",
		"SCARLET",
		"SCARLET-NAS",
		"ScatNet",
		"Scatter Connection",
		"SCCL",
		"Scene Text Models",
		"ScheduledDropPath",
		"SchNet",
		"SCN",
		"SCNet",
		"SCNN_UNet_ConvLSTM",
		"scSE",
		"SCST",
		"SDAE",
		"SDNE",
		"SEAM",
		"SEED RL",
		"SEER",
		"Seesaw Loss",
		"SegFormer",
		"SegNet",
		"Segregated Attention Network",
		"SegSort",
		"Selective Kernel Convolution",
		"Selective Kernel",
		"Selective Search",
		"Self-adaptive Training",
		"Self-Adjusting Smooth L1 Loss",
		"Self-Adversarial Negative Sampling",
		"Self-Attention Guidance",
		"Self-Calibrated Convolutions",
		"Self-Learning",
		"self-mem + new data",
		"Self-Supervised Learning",
		"Self-Training Methods",
		"SELU",
		"Semantic Reasoning Network",
		"Semantic Segmentation Models",
		"Semantic Segmentation Modules",
		"Semi-Supervised Learning Methods",
		"SENet",
		"Sentence Embeddings",
		"SentencePiece",
		"Separate And Diffuse",
		"SepFormer",
		"Seq2Edits",
		"Seq2Seq",
		"Sequence Decoding Methods",
		"Sequence Editing Models",
		"Sequence To Sequence Models",
		"Sequential Blocks",
		"Sequential",
		"SERAC",
		"Serf",
		"SERLU",
		"SESAME Discriminator",
		"Set Transformer",
		"SETR",
		"SETSe",
		"SFAM",
		"SFT",
		"SGD with Momentum",
		"SGD",
		"SGDW",
		"SGPCS",
		"SHA-RNN",
		"Shake-Shake Regularization",
		"ShakeDrop",
		"SHAP",
		"Shape Adaptor",
		"ShapeConv",
		"Sharded Data Parallel Methods",
		"Sharpness-Aware Minimization",
		"Shifted Softplus",
		"ShiLU",
		"Shuffle-T",
		"ShuffleNet Block",
		"ShuffleNet V2 Block",
		"ShuffleNet V2 Downsampling Block",
		"ShuffleNet v2",
		"ShuffleNet",
		"Siamese Network",
		"Siamese U-Net",
		"SIFA",
		"SIG",
		"Sigmoid Activation",
		"SiLU",
		"SimAdapter",
		"SimAug",
		"SimCLR",
		"SimCLRv2",
		"SimCSE",
		"SimpleNet",
		"SimVLM",
		"Single-Headed Attention",
		"Single-path NAS",
		"Singular Value Clipping",
		"Sinkhorn Transformer",
		"Siren",
		"SIRM",
		"SKEP",
		"Skip Connection Blocks",
		"Skip Connections",
		"Skip-gram Word2Vec",
		"SkipInit",
		"SKNet",
		"SLAM Methods",
		"SLAMB",
		"Slanted Triangular Learning Rates",
		"Sliding Window Attention",
		"Slot Attention",
		"SlowMo",
		"SLR",
		"SM3",
		"SMA",
		"SmeLU",
		"Smish",
		"SMITH",
		"Smooth Step",
		"SMOT",
		"SMOTE",
		"SNAIL",
		"Snapshot Ensembles",
		"SNet",
		"SNGAN",
		"SNIP",
		"SNIPER",
		"Social-STGCNN",
		"Soft Actor Critic",
		"Soft Actor-Critic (Autotuned Temperature)",
		"Soft Split and Soft Composition",
		"Soft-NMS",
		"Softmax",
		"Softplus",
		"SoftPool",
		"Softsign Activation",
		"SOHO",
		"SOM",
		"SongNet",
		"SortCut Sinkhorn Attention",
		"Soups",
		"Source Hypothesis Transfer",
		"SPADE",
		"Span Representations",
		"Span-Based Dynamic Convolution",
		"Sparse Autoencoder",
		"Sparse Convolutions",
		"Sparse R-CNN",
		"Sparse Sinkhorn Attention",
		"Sparse Switchable Normalization",
		"Sparse Transformer",
		"Sparsemax",
		"Sparsity",
		"Spatial & Temporal Attention",
		"Spatial Attention Module (ThunderNet)",
		"Spatial Attention Module",
		"Spatial Attention-Guided Mask",
		"Spatial Broadcast Decoder",
		"Spatial Feature Transform",
		"Spatial Gating Unit",
		"Spatial Group-wise Enhance",
		"Spatial Propagation",
		"Spatial Pyramid Pooling",
		"Spatial Transformer",
		"Spatial-Reduction Attention",
		"SpatialDropout",
		"Spatially Separable Convolution",
		"Spatially Separable Self-Attention",
		"Speaker Diarization",
		"SpecGAN",
		"Spectral Clustering",
		"Spectral DeTuning",
		"Spectral Dropout",
		"Spectral Normalization",
		"Spectral-Normalized Identity Priors",
		"Speech Embeddings",
		"Speech enhancement",
		"Speech Recognition",
		"Speech Separation Models",
		"Speech Synthesis Blocks",
		"SPIN",
		"SpineNet",
		"SPL",
		"Split Attention",
		"SPNet",
		"SPP-Net",
		"SPPF-AMP",
		"Spreadsheet Formula Prediction Models",
		"SpreadsheetCoder",
		"SPS",
		"Squared ReLU",
		"Squeeze-and-Excitation Block",
		"SqueezeBERT",
		"SqueezeNet",
		"SqueezeNeXt Block",
		"SqueezeNeXt",
		"srBTAW (BTW)",
		"SRDC",
		"SReLU",
		"SRGAN Residual Block",
		"SRGAN",
		"SRM",
		"SRMM",
		"SRN",
		"SRS",
		"SRU",
		"SRU++",
		"Sscs",
		"SSD",
		"SSDS",
		"SSE",
		"SSFG regularization",
		"SSKD",
		"SSTDA",
		"STA-LSTM",
		"STAC",
		"Stacked Hourglass Network",
		"StarReLU",
		"State Similarity Metrics",
		"State-Aware Tracker",
		"STATEGAME MAINTAIN PICTURE BALANCED PLAY STABLE",
		"Static Word Embeddings",
		"Statistical Inference",
		"STD",
		"STDC",
		"Step Decay",
		"Stereo Depth Estimation Models",
		"StereoLayers",
		"STMDA-RetinaNet",
		"STN",
		"Stochastic Depth",
		"Stochastic Dueling Network",
		"Stochastic Gradient Variational Bayes",
		"Stochastic Optimization",
		"Stochastic Weight Averaging",
		"StoGCN",
		"STraTA",
		"Streaming Module",
		"StreaMRAK",
		"Strided Attention",
		"Strided EESP",
		"Strip Pooling",
		"StruBERT",
		"Structured Prediction",
		"STTP",
		"Style Transfer Models",
		"Style Transfer Module",
		"Style Transfer Modules",
		"Style-based Recalibration Module",
		"StyleALAE",
		"StyleGAN",
		"StyleGAN2",
		"StyleMapGAN",
		"StyleSwin",
		"Subformer",
		"Submanifold Convolution",
		"Subword Segmentation",
		"Super-Resolution Models",
		"SuperpixelGridMasks",
		"Supervised Contrastive Loss",
		"SVD Parameterization",
		"SVM",
		"SVPG",
		"SwAV",
		"SwiGLU",
		"Swin Transformer",
		"Swish",
		"Switch FFN",
		"Switch Transformer",
		"Switchable Normalization",
		"SWRNet",
		"SyCoCa",
		"Sym-NCO",
		"Symbolic Deep Learning",
		"Symbolic rule learning",
		"SymmNet",
		"SynaNN",
		"SyncBN",
		"Synchronous Pipeline Parallel",
		"Syntax Heat Parse Tree",
		"Synthesized Attention Mechanisms",
		"Synthesizer",
		"T-D",
		"T-Fixup",
		"T2T-ViT",
		"T5",
		"TABBIE",
		"TaBERT",
		"Table Parsing Models",
		"Table Question Answering Models",
		"TabNet",
		"TabNN",
		"TABPFN",
		"TabTransformer",
		"Tabular Data Generation",
		"Tacotron 2",
		"Tacotron",
		"TaLK Convolution",
		"Talking-Heads Attention",
		"TAM",
		"Tanh Activation",
		"TanhExp",
		"TAPAS",
		"TAPEX",
		"Target Policy Smoothing",
		"Targeted Dropout",
		"TaxoExpan",
		"Taxonomy Expansion Models",
		"TayPO",
		"TD Lambda",
		"TD-Gammon",
		"TD-VAE",
		"TD3",
		"TDN",
		"TE2Rules",
		"Teacher-Tutor-Student Knowledge Distillation",
		"Temporal Activation Regularization",
		"Temporal attention",
		"Temporal Convolutions",
		"Temporal Distribution Characterization",
		"Temporal Distribution Matching",
		"Temporal Jittering",
		"Temporal ROIAlign",
		"Temporally Consistent Spatial Augmentation",
		"Ternarization",
		"Ternary Weight Splitting",
		"TernaryBERT",
		"Text Augmentation",
		"Text Classification Models",
		"Text Data Augmentation",
		"Text Instance Representations",
		"Text-to-Speech Models",
		"Textual Inference Models",
		"Textual Meaning",
		"TFGW",
		"TGAN",
		"TGN",
		"Theorem Proving Models",
		"Thermal Image Processing Models",
		"ThunderNet",
		"TILDEv2",
		"Time Series Analysis",
		"Time Series Modules",
		"time-caus-scsp",
		"timecausgabor",
		"timecauslimitkernel",
		"TimeSformer",
		"TinaFace",
		"TinyNet",
		"TLC",
		"TNT",
		"Tofu",
		"Tokenizers",
		"Topic Embeddings",
		"TopK Copy",
		"Topographic VAE",
		"TorchBeast",
		"TPN",
		"TraDeS",
		"Trajectory Data Augmentation",
		"Trajectory Prediction Models",
		"Trans-Encoder",
		"Transductive Inference",
		"TransE",
		"TransferQA",
		"Transformer",
		"Transformer-XL",
		"Transformers",
		"Transposed convolution",
		"Tree-structured Parzen Estimator Approach (TPE)",
		"TResNet",
		"TridentNet Block",
		"TridentNet",
		"Triplet Attention",
		"Triplet Entropy Loss",
		"Triplet Loss",
		"TrIVD-GAN",
		"TrOCR",
		"TRPO",
		"True Online TD Lambda",
		"Truncation Trick",
		"TS",
		"TSDAE",
		"TSRUc",
		"TSRUp",
		"TSRUs",
		"TTUR",
		"TuckER",
		"TuckER-RP",
		"TUM",
		"Tunable Network",
		"TURL",
		"TWEC",
		"Twin Networks",
		"Twins-PCPVT",
		"Twins-SVT",
		"Two-Way Dense Layer",
		"U-CAM",
		"U-Net GAN",
		"U-Net",
		"U-RNNs",
		"U2-Net",
		"UCNet",
		"UCTransNet",
		"UFLoss",
		"UL2",
		"ULMFiT",
		"UNet++",
		"UNETR",
		"uNetXST",
		"Unified VLP",
		"Unigram Segmentation",
		"UNIMO",
		"Unitary RNN",
		"UNITER",
		"Universal Probing",
		"Universal Transformer",
		"Unpaired Image-to-Image Translation",
		"UORO",
		"uPIT",
		"USE",
		"V-trace",
		"VAE",
		"Value Function Estimation",
		"Variational Dropout",
		"Variational Entanglement Detection",
		"Variational Inference",
		"Variational Optimization",
		"Varifocal Loss",
		"VarImpVIANN",
		"VATT",
		"VC R-CNN",
		"VDO-SLAM",
		"VEGA",
		"VERSE",
		"VFNet",
		"VGAE",
		"VGG Loss",
		"VGG",
		"VGG-16",
		"VGG-19",
		"VGGoptiVMD",
		"Video Data Augmentation",
		"Video Frame Interpolation",
		"Video Game Models",
		"Video Inpainting Models",
		"Video Instance Segmentation Models",
		"Video Interpolation Models",
		"Video Model Blocks",
		"Video Object Segmentation Models",
		"Video Panoptic Segmentation Models",
		"Video Quality Models",
		"Video Recognition Models",
		"Video Sampling",
		"Video Super-Resolution Models",
		"Video-Text Retrieval Models",
		"VideoBERT",
		"Viewmaker Network",
		"ViLBERT",
		"ViLT",
		"VIME",
		"ViP-DeepLab",
		"VirTex",
		"Virtual Batch Normalization",
		"Virtual Data Augmentation",
		"Visformer",
		"Vision and Language Pre-Trained Models",
		"Vision Transformer",
		"Vision Transformers",
		"Vision-aided GAN",
		"VisTR",
		"Visual Analytics",
		"Visual Attention",
		"Visual Parsing",
		"VisualBERT",
		"VL-BERT",
		"VL-T5",
		"VLG-Net",
		"VLMo",
		"VocGAN",
		"VoiceFilter-Lite",
		"Vokenization",
		"VOS",
		"VoTr",
		"VoVNet",
		"VoVNetV2",
		"Voxel R-CNN",
		"Voxel RoI Pooling",
		"VPSNet",
		"VQ-VAE",
		"VQ-VAE-2",
		"VQA Models",
		"VQSVD",
		"VSF",
		"VSGNet",
		"VTDE",
		"Vulnerability-constrained Decoding",
		"wav2vec-U",
		"WaveGAN",
		"WaveGlow",
		"WaveGrad DBlock",
		"WaveGrad UBlock",
		"WaveGrad",
		"Wavelet Distributed Training",
		"WaveNet",
		"WaveRNN",
		"WaveTTS",
		"WaveVAE",
		"Webpage Object Detection Pipeline",
		"WEGL",
		"Weight Decay",
		"Weight Demodulation",
		"Weight excitation",
		"Weight Normalization",
		"Weight Standardization",
		"Weight Tying",
		"Weights Reset",
		"WenLan",
		"WFST",
		"WGAN GP",
		"WGAN",
		"WGAN-GP Loss",
		"Whitening",
		"Wide Residual Block",
		"Wide&Deep",
		"WideResNet",
		"Window-based Discriminator",
		"WIPA",
		"Wizard",
		"Word Embeddings",
		"WordPiece",
		"Working Memory Models",
		"WRQE",
		"WYS",
		"Xavier Initialization",
		"Xception",
		"XCiT Layer",
		"XCiT",
		"XGPT",
		"XGrad-CAM",
		"XLM",
		"XLM-R",
		"XLNet",
		"XLSR",
		"YellowFin",
		"YOHO",
		"YOLOP",
		"YOLOv1",
		"YOLOv2",
		"YOLOv3",
		"YOLOv4",
		"YOLOX",
		"Z-PNN",
		"ZCA Whitening",
		"ZeRO",
		"ZeRO-Infinity",
		"ZeRO-Offload",
		"Zero-padded Shortcut Connection",
		"ZFNet",
		"Zoneout",
		"ZoomNet"
	],
	"examples": [
		"Contrastive Learning",
		"UNet++"
	]
}